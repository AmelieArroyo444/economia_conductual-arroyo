---
title: "experimento_pokemon.qmd"
---

---
title: "Pok√©mon Probabilities"
format: html
execute:
  echo: true
  eval: true
  engine: knitr
  freeze: auto
  cache: false
  warning: false
  message: false
  error: true
  # Define Python path if needed
  # python: "/ruta/a/tu/python"
---

### üîÅ **Pol√≠tica**

Una pol√≠tica es la regla que un agente sigue para **decidir qu√© acci√≥n tomar** en cada estado. Es clave para **maximizar recompensas a largo plazo**. Es como la estrategia del jugador en un videojuego.

------------------------------------------------------------------------

### üé∞ **El problema del bandido de m√∫ltiples brazos**

Imagina una m√°quina con varios botones (brazos), cada uno con distinta probabilidad de darte una recompensa. Para saber cu√°l es el mejor, necesitas **probar todos suficientes veces**, pero si **exploras demasiado, pierdes tiempo para explotar el mejor**. Ah√≠ nace...

------------------------------------------------------------------------

### ‚öñÔ∏è **Dilema Exploraci√≥n - Explotaci√≥n**

Un cl√°sico: ¬øprobar nuevas opciones (explorar) o seguir con la que ya conoces que funciona (explotar)?

üîπ **Explorar mucho** = Menos recompensas inmediatas.

üîπ **Explotar r√°pido** = Riesgo de perder mejores opciones.

------------------------------------------------------------------------

### üß† **Valores Q**

Son las "expectativas de recompensa" por tomar una acci√≥n en un estado. Pero **saber el valor Q no basta**: el agente debe convertir esos valores en acciones reales mediante una **regla de elecci√≥n**.

------------------------------------------------------------------------

### üéØ **Reglas de elecci√≥n**

Son f√≥rmulas o algoritmos que **transforman valores Q en decisiones**. Te ayudan a decidir entre m√∫ltiples opciones.

------------------------------------------------------------------------

### üíé **Regla de Maximizaci√≥n (Greedy)**

Selecciona **siempre la acci√≥n con mayor Q**.

‚ùå Problemas:

-   

-   Las personas **no eligen siempre lo mejor** (variabilidad emp√≠rica).

-   

-   Se puede **quedar atrapado en un m√°ximo local**, sin explorar mejores opciones.

-   

------------------------------------------------------------------------

### ‚ö° **Regla √©psilon-codiciosa**

Soluci√≥n para el dilema exploraci√≥n/explotaci√≥n:

-   

-   Con **probabilidad (1 - Œµ)** elige la mejor acci√≥n.

-   

-   Con **probabilidad Œµ**, elige aleatoriamente.

-   

üî¢ Ejemplo:

-   

-   Œµ = 0.1: Explora el 10% del tiempo.

-   

-   Œµ = 0.01: Explora menos, pero puede encontrar mejores resultados a largo plazo.

-   

------------------------------------------------------------------------

### üß™ **Funciones de respuesta probabil√≠sticas**

Modelos de psicolog√≠a que **explican elecciones aleatorias**, especialmente cuando las diferencias entre opciones no son tan claras. Aqu√≠ entran:

------------------------------------------------------------------------

### üìä **Funciones psicom√©tricas**

Muestran c√≥mo, cuando las diferencias entre est√≠mulos (o Qs) son peque√±as, las elecciones se vuelven menos consistentes. Se derivan dos modelos:

------------------------------------------------------------------------

### üß© **Modelos de Utilidad Aleatoria**

#### ‚úÖ **Funci√≥n Probit**

-   

-   Supone que Q es **una variable aleatoria** (como una sensaci√≥n).

-   

-   Usa la **distribuci√≥n normal acumulada** para calcular la probabilidad de elegir una opci√≥n.

-   

#### ‚úÖ **Funci√≥n Log√≠stica (Logit o Softmax)**

-   

-   M√°s usada en aprendizaje autom√°tico.

-   

-   Transforma la diferencia entre Qs usando la funci√≥n log√≠stica:

    P(a1)=11+e‚àíŒª(Qa1‚àíQa2)P(a_1) = \frac{1}{1 + e^{-\lambda(Q_{a1} - Q_{a2})}}P(a1‚Äã)=1+e‚àíŒª(Qa1‚Äã‚àíQa2‚Äã)1‚Äã

-   

-   El par√°metro **Œª** controla la sensibilidad a las diferencias en Q.

    -   

    -   Œª ‚Üí 0 = elecciones casi aleatorias.

    -   

    -   Œª ‚Üí ‚àû = elecciones deterministas (como la greedy).

    -   

-   

------------------------------------------------------------------------

### üíñ **Modelo de Luce (Elecci√≥n Proporcional)**

-   

-   No considera que Q sea aleatorio.

-   

-   Elige acciones **proporcionalmente a su valor Q**:

    P(a1)=Q(a1)Q(a1)+Q(a2)P(a_1) = \frac{Q(a_1)}{Q(a_1) + Q(a_2)}P(a1‚Äã)=Q(a1‚Äã)+Q(a2‚Äã)Q(a1‚Äã)‚Äã

    Tiene una versi√≥n con Œª que modula la sensibilidad, igual que el softmax.

-   

------------------------------------------------------------------------

### ‚ú® En resumen:

| Concepto                       | ¬øQu√© hace?                               |
|--------------------------------|------------------------------------------|
| **Pol√≠tica**                   | Decide qu√© acci√≥n tomar.                 |
| **Exploraci√≥n vs Explotaci√≥n** | Dilema sobre probar o repetir acciones.  |
| **Greedy (Maximizaci√≥n)**      | Siempre elige la mejor acci√≥n (seg√∫n Q). |
| **Œµ-codicioso**                | Mayor√≠a greedy, pero a veces explora.    |
| **Probit y Logit**             | Modelos probabil√≠sticos de elecci√≥n.     |
| **Softmax**                    | Regla log√≠stica para m√∫ltiples opciones. |
| **Luce (Acci√≥n proporcional)** | Elige basado en proporci√≥n del valor Q.  |

------------------------------------------------------------------------
